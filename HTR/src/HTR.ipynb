{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import warnings\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "import warnings\n",
    "import traceback\n",
    "from path import Path\n",
    "import random\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from imutils import contours\n",
    "import editdistance\n",
    "\n",
    "import pytesseract\n",
    "from pytesseract import Output\n",
    "\n",
    "from DataLoaderIAM import DataLoaderIAM, Batch\n",
    "from Model import Model, DecoderType\n",
    "from SamplePreprocessor import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.autograph.set_verbosity(0)\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePaths:\n",
    "    \"filenames and paths to data\"\n",
    "    fnCharList = '../model/charList.txt'\n",
    "    fnSummary = '../model/summary.json'\n",
    "    fnInfer = '../data/test.png'\n",
    "    fnCorpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def write_summary(charErrorRates, wordAccuracies):\n",
    "    with open(FilePaths.fnSummary, 'w') as f:\n",
    "        json.dump({'charErrorRates': charErrorRates, 'wordAccuracies': wordAccuracies}, f)\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"train NN\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summaryCharErrorRates = []\n",
    "    summaryWordAccuracies = []\n",
    "    bestCharErrorRate = float('inf')  # best valdiation character error rate\n",
    "    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\n",
    "    earlyStopping = 25  # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.trainSet()\n",
    "        while loader.hasNext():\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            batch = loader.getNext()\n",
    "            loss = model.trainBatch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iterInfo[0]}/{iterInfo[1]} Loss: {loss}')\n",
    "\n",
    "        # validate\n",
    "        charErrorRate, wordAccuracy = validate(model, loader)\n",
    "\n",
    "        # write summary\n",
    "        summaryCharErrorRates.append(charErrorRate)\n",
    "        summaryWordAccuracies.append(wordAccuracy)\n",
    "        write_summary(summaryCharErrorRates, summaryWordAccuracies)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Character error rate improved, save model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovementSince = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {charErrorRate * 100.0}%')\n",
    "            noImprovementSince += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if noImprovementSince >= earlyStopping:\n",
    "            print(f'No more improvement since {earlyStopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    \"validate NN\"\n",
    "    print('Validate NN')\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print(f'Batch: {iterInfo[0]} / {iterInfo[1]}')\n",
    "        batch = loader.getNext()\n",
    "        (recognized, _) = model.inferBatch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "            numWordTotal += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtTexts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gtTexts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    charErrorRate = numCharErr / numCharTotal\n",
    "    wordAccuracy = numWordOK / numWordTotal\n",
    "    print(f'Character error rate: {charErrorRate * 100.0}%. Word accuracy: {wordAccuracy * 100.0}%.')\n",
    "    return charErrorRate, wordAccuracy\n",
    "\n",
    "# Create ground truth reference\n",
    "gt = pd.read_csv('../../collectedData/labels.csv')\n",
    "gt.text = gt.text.apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(file):\n",
    "    # configure tesseract\n",
    "    ## oem 3 == Engine Mode: Default, based on what is available (Legacy, LSTM, or both)\n",
    "    ## psm 3 == Page Segmentation Mode: Fully automatic page segmentation, but no OSD\n",
    "    ## psm 11 == Page Segmentation Mode: Sparse text - find as much text as possible in no particular order, no OSD\n",
    "    custom_config = r'--oem 3 --psm 11'\n",
    "\n",
    "#     inPath = '../../collectedData/raw/'\n",
    "    outParent = '../../collectedData/sliced/'\n",
    "\n",
    "#     for file in os.listdir(inPath):    \n",
    "    outPath = os.path.join(outParent,'.'.join(file.split('.')[:-1]))\n",
    "    print(f'\\nSplitting < {file} > -> < {outPath} >')\n",
    "    if not os.path.exists(outPath):\n",
    "        os.mkdir(outPath)\n",
    "\n",
    "    img = cv2.imread(os.path.join(inPath,file))\n",
    "    imgMarked = cv2.imread(os.path.join(inPath,file))\n",
    "    imgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    imgBinary = cv2.threshold(imgGray, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\n",
    "\n",
    "    details = pytesseract.image_to_data(imgBinary, output_type=Output.DICT, config=custom_config, lang='eng')\n",
    "    total_boxes = len(details['text'])\n",
    "    for sequence_number in range(total_boxes):\n",
    "        if int(details['conf'][sequence_number]) > -1:\n",
    "            (x, y, w, h) = (details['left'][sequence_number], details['top'][sequence_number], details['width'][sequence_number],  details['height'][sequence_number])\n",
    "            imgBinary = cv2.rectangle(imgBinary, (x, y), (x + w, y + h), (0, 0, 0), 2)\n",
    "            imgMarked = cv2.rectangle(imgMarked, (x, y), (x + w, y + h), (random.randint(0,255), random.randint(0,255), random.randint(0,255)), 2)\n",
    "            crop_img = img[y:y+h, x:x+w]\n",
    "            cv2.imwrite(os.path.join(outPath,str(sequence_number).zfill(len(str(total_boxes)))+'.png'), crop_img)\n",
    "\n",
    "    cv2.imwrite(os.path.join(outParent,'_markings',str(file)), imgMarked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show(img, text='img'):\n",
    "    cv2.imshow(str(text), img)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeCnts(cnts):\n",
    "    corners = []\n",
    "    for c in cnts:\n",
    "        pts = [list(x[0]) for x in c]\n",
    "        xVals = [x[0] for x in pts]\n",
    "        yVals = [x[1] for x in pts]\n",
    "        xMin = min(xVals)\n",
    "        xMax = max(xVals)\n",
    "        yMin = min(yVals)\n",
    "        yMax = max(yVals)\n",
    "\n",
    "        for i, shape in enumerate(corners):\n",
    "            left = shape[0][0,0]\n",
    "            right = shape[1][0,0]\n",
    "            top = shape[0][0,1]\n",
    "            bottom = shape[-1][0,1]\n",
    "            if (xMin >= left and xMin <= right) or (xMax >= left and xMax <= right):\n",
    "                xMin = min(xMin,left)\n",
    "                xMax = max(xMax,right)\n",
    "                yMin = min(yMin,top)\n",
    "                yMax = max(yMax,bottom)\n",
    "                del corners[i]\n",
    "\n",
    "        corners.append(np.array([[[xMin,yMin]],[[xMax,yMin]],[[xMax,yMax]],[[xMin,yMax]]]))\n",
    "    \n",
    "    return corners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(path):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    sys.argv = []\n",
    "    parser = argparse.ArgumentParser(sys.argv)\n",
    "    parser.add_argument('--train', help='train the NN', action='store_true')\n",
    "    parser.add_argument('--validate', help='validate the NN', action='store_true')\n",
    "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath',\n",
    "                        help='CTC decoder')\n",
    "    parser.add_argument('--batch_size', help='batch size', type=int, default=100)\n",
    "    parser.add_argument('--data_dir', help='directory containing IAM dataset', type=Path, required=False)\n",
    "    parser.add_argument('--fast', help='use lmdb to load images', action='store_true')\n",
    "    parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "#     args.validate = True\n",
    "    args.data_dir = Path(path)\n",
    "    print(f'Predicting on slices in < {path} >')\n",
    "    debug = False\n",
    "    if debug:\n",
    "        print(\"\\n********\")\n",
    "        print(args)\n",
    "        print(vars(args))\n",
    "        print(args.data_dir)\n",
    "        print(\"********\\n\")\n",
    "\n",
    "    \n",
    "    # set chosen CTC decoder\n",
    "    if args.decoder == 'bestpath':\n",
    "        decoderType = DecoderType.BestPath\n",
    "    elif args.decoder == 'beamsearch':\n",
    "        decoderType = DecoderType.BeamSearch\n",
    "    elif args.decoder == 'wordbeamsearch':\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    # train or validate on IAM dataset\n",
    "    if args.train or args.validate:\n",
    "        # load training data, create TF model\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, Model.imgSize, Model.maxTextLen, args.fast)\n",
    "\n",
    "        # save characters of model for inference mode\n",
    "        open(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\n",
    "\n",
    "        # save words contained in dataset into file\n",
    "        open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "        # execute training or validation\n",
    "        if args.train:\n",
    "            model = Model(loader.charList, decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            model = Model(loader.charList, decoderType, mustRestore=True)\n",
    "            validate(model, loader)\n",
    "\n",
    "    # infer text on test images\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['Actual','Guess','Conf','Result','Dist','Full Text','ResMatch','Path'])\n",
    "        model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\n",
    "        gt = pd.read_csv('../../collectedData/labels.csv')\n",
    "        gt.text = gt.text.apply(lambda x: x.split())\n",
    "        for sample in os.listdir(args.data_dir):\n",
    "            if 'breakdowns' in sample:\n",
    "                continue\n",
    "            \n",
    "            guess, conf = infer(model, os.path.join(args.data_dir,sample))\n",
    "            \n",
    "            actual = re.sub('((\\W+\\(\\d*\\))?\\.((png)|(jpg)))','',sample)\n",
    "            actual = actual.split(' ',1)[-1]\n",
    "            result = actual==guess\n",
    "            fullText = gt[gt.filename.str.contains(path.split('/')[-1])].iloc[0,1]\n",
    "            resMatch = guess in fullText\n",
    "            if len(actual)>0:\n",
    "                dist = editdistance.eval(guess, actual)/len(actual)\n",
    "            else:\n",
    "                dist=len(guess)\n",
    "#                 print('Actual:\\t', actual)\n",
    "#                 print('Guess:\\t', guess)\n",
    "#                 print('Conf:\\t ', conf)\n",
    "#                 print('\\t', result)\n",
    "#                 print()\n",
    "            \n",
    "            df = df.append(pd.Series([actual, guess, conf, result, dist, fullText, resMatch, os.path.join(args.data_dir,sample)], index = df.columns), ignore_index=True)\n",
    "            \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-cb3970c5b870>, line 40)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-8-cb3970c5b870>\"\u001b[1;36m, line \u001b[1;32m40\u001b[0m\n\u001b[1;33m    crop_img = img[y:y+h, x:x+w]git\u001b[0m\n\u001b[1;37m                                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def infer1(model, img):\n",
    "    scaleFactor = 20/max(img.shape[:2])\n",
    "    width = int(img.shape[1] * scaleFactor)\n",
    "    height = int(img.shape[0] * scaleFactor)\n",
    "    dim = (width, height)\n",
    "\n",
    "    scaled = cv2.resize(img,dim)\n",
    "\n",
    "    canvas = np.zeros((28,28), dtype = \"uint8\")\n",
    "\n",
    "    x = int((28 - scaled.shape[1])/2)\n",
    "    y = int((28 - scaled.shape[0])/2)\n",
    "    canvas[y:y+scaled.shape[0],x:x+scaled.shape[1]] = scaled\n",
    "\n",
    "#     print(model.predict(canvas.reshape(1, 28, 28, 1)))\n",
    "    return str(model.predict(canvas.reshape(1, 28, 28, 1)).argmax())\n",
    "\n",
    "def breakdown(raw):\n",
    "    model = keras.models.load_model('../model/charModel')\n",
    "    \n",
    "    blur = cv2.GaussianBlur(raw, (9, 9), 0)\n",
    "    thresh = cv2.threshold(blur,0,255,cv2.THRESH_OTSU + cv2.THRESH_BINARY)[1]\n",
    "    invert = cv2.bitwise_not(thresh)\n",
    "    img = invert\n",
    "\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 1))\n",
    "    dilate = cv2.dilate(img, kernel, iterations=5)\n",
    "\n",
    "    contours, hierarchy = cv2.findContours(dilate, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnts = sorted(contours, key = cv2.contourArea, reverse = True)\n",
    "    cnts = mergeCnts(cnts)\n",
    "\n",
    "    guess = ''\n",
    "    for i, c in enumerate(cnts):\n",
    "        area = cv2.contourArea(c)\n",
    "        if area > 10:\n",
    "            x,y,w,h = cv2.boundingRect(c)\n",
    "            ROI = 255 - raw[y:y+h, x:x+w]\n",
    "            cv2.rectangle(raw, (x, y), (x + w, y + h), (random.randint(0,255),random.randint(0,255),random.randint(0,255)), 1)\n",
    "            crop_img = img[y:y+h, x:x+w]           \n",
    "            guess += infer1(model, crop_img)\n",
    "                        \n",
    "    if len(cnts) > 1:\n",
    "        return(guess)\n",
    "    else:\n",
    "        return('')\n",
    "\n",
    "def infer(model, imgPath):    \n",
    "    \"recognize text in image provided by file path\"\n",
    "    raw = cv2.imread(imgPath, cv2.IMREAD_GRAYSCALE)\n",
    "    img = preprocess(raw, Model.imgSize)\n",
    "    batch = Batch(None, [img])\n",
    "    \n",
    "    (recognized, probability) = model.inferBatch(batch, True)\n",
    "    \n",
    "    if probability < .3:\n",
    "        newGuess = breakdown(raw)\n",
    "        \n",
    "        if len(newGuess)>0:\n",
    "            probability[0] = -1\n",
    "            recognized[0] = newGuess\n",
    "        \n",
    "#         print('Done', imgPath)\n",
    "#         print(f'\\tRecognized: \"{recognized[0]}\"')\n",
    "#         print(f'\\tProbability: {probability[0]}')\n",
    "#         print()\n",
    "        \n",
    "    return (recognized[0],probability[0])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    out = {}\n",
    "    #for path in [os.path.join('../../collectedData/sliced/',x) for x in os.listdir('../../collectedData/sliced/')]:\n",
    "    inPath = '../../collectedData/raw/'\n",
    "    for file in os.listdir(inPath):\n",
    "        name = file[:-4]\n",
    "        if '_markings' in file:\n",
    "            continue\n",
    "        \n",
    "        split(file)\n",
    "        try:  \n",
    "            out[name] = main(os.path.join('../../collectedData/sliced/',name))\n",
    "        except:\n",
    "            print(f'\\tERROR on: < {file} >')\n",
    "#             traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in out.keys():\n",
    "    print(key)\n",
    "    df = out[key]\n",
    "    actual = gt[gt.filename.str.contains(key)].iloc[0,1]\n",
    "    orig = actual\n",
    "    correct = []\n",
    "    for word in df[df.ResMatch].Guess:\n",
    "        if word in actual:\n",
    "            correct.append(word)\n",
    "            actual = actual[actual.index(word)+1:]\n",
    "    print(correct)\n",
    "    print(f'{len(correct)}/{len(orig)}: {len(correct)/len(orig)}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Stats based on individually labeled slice folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out['math1'] = main('../../collectedData/sliced/math1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = out['math1']\n",
    "\n",
    "print(df.Result.value_counts())\n",
    "print()\n",
    "\n",
    "print(f'Accuracy: {sum(df.Result == True)/len(df.Result)}')\n",
    "print(f'Character Error Rate: {df.Dist.mean()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[df.Result==False].Conf, color='red', bins=20, alpha=0.75)\n",
    "plt.hist(df[df.Result==True].Conf, color='green', bins=20, alpha=0.5)\n",
    "plt.ylabel('# of Samples')\n",
    "plt.xlabel('Confidence')\n",
    "plt.legend(['Incorrect','Correct'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for tup in df.itertuples():\n",
    "    im = Image.open(tup.Path)\n",
    "    imshow(np.asarray(im))\n",
    "    plt.show()\n",
    "    print('Actual:\\t', tup.Actual)\n",
    "    print('Guess:\\t', tup.Guess)\n",
    "    print('Conf:\\t ', tup.Conf)\n",
    "    print('Dist:\\t ', tup.Dist)\n",
    "    print('\\t', tup.Result)\n",
    "    print('\\n***************************************************************************************************************')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
