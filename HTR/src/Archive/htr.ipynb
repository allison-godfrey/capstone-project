{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "\n",
    "import argparse\n",
    "import json\n",
    "\n",
    "import cv2\n",
    "import editdistance\n",
    "from path import Path\n",
    "\n",
    "from DataLoaderIAM import DataLoaderIAM, Batch\n",
    "from Model import Model, DecoderType\n",
    "from SamplePreprocessor import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FilePaths:\n",
    "    \"filenames and paths to data\"\n",
    "    fnCharList = '../model/charList.txt'\n",
    "    fnSummary = '../model/summary.json'\n",
    "    fnInfer = '../data/test.png'\n",
    "    fnCorpus = '../data/corpus.txt'\n",
    "\n",
    "\n",
    "def write_summary(charErrorRates, wordAccuracies):\n",
    "    with open(FilePaths.fnSummary, 'w') as f:\n",
    "        json.dump({'charErrorRates': charErrorRates, 'wordAccuracies': wordAccuracies}, f)\n",
    "\n",
    "\n",
    "def train(model, loader):\n",
    "    \"train NN\"\n",
    "    epoch = 0  # number of training epochs since start\n",
    "    summaryCharErrorRates = []\n",
    "    summaryWordAccuracies = []\n",
    "    bestCharErrorRate = float('inf')  # best valdiation character error rate\n",
    "    noImprovementSince = 0  # number of epochs no improvement of character error rate occured\n",
    "    earlyStopping = 25  # stop training after this number of epochs without improvement\n",
    "    while True:\n",
    "        epoch += 1\n",
    "        print('Epoch:', epoch)\n",
    "\n",
    "        # train\n",
    "        print('Train NN')\n",
    "        loader.trainSet()\n",
    "        while loader.hasNext():\n",
    "            iterInfo = loader.getIteratorInfo()\n",
    "            batch = loader.getNext()\n",
    "            loss = model.trainBatch(batch)\n",
    "            print(f'Epoch: {epoch} Batch: {iterInfo[0]}/{iterInfo[1]} Loss: {loss}')\n",
    "\n",
    "        # validate\n",
    "        charErrorRate, wordAccuracy = validate(model, loader)\n",
    "\n",
    "        # write summary\n",
    "        summaryCharErrorRates.append(charErrorRate)\n",
    "        summaryWordAccuracies.append(wordAccuracy)\n",
    "        write_summary(summaryCharErrorRates, summaryWordAccuracies)\n",
    "\n",
    "        # if best validation accuracy so far, save model parameters\n",
    "        if charErrorRate < bestCharErrorRate:\n",
    "            print('Character error rate improved, save model')\n",
    "            bestCharErrorRate = charErrorRate\n",
    "            noImprovementSince = 0\n",
    "            model.save()\n",
    "        else:\n",
    "            print(f'Character error rate not improved, best so far: {charErrorRate * 100.0}%')\n",
    "            noImprovementSince += 1\n",
    "\n",
    "        # stop training if no more improvement in the last x epochs\n",
    "        if noImprovementSince >= earlyStopping:\n",
    "            print(f'No more improvement since {earlyStopping} epochs. Training stopped.')\n",
    "            break\n",
    "\n",
    "\n",
    "def validate(model, loader):\n",
    "    \"validate NN\"\n",
    "    print('Validate NN')\n",
    "    loader.validationSet()\n",
    "    numCharErr = 0\n",
    "    numCharTotal = 0\n",
    "    numWordOK = 0\n",
    "    numWordTotal = 0\n",
    "    while loader.hasNext():\n",
    "        iterInfo = loader.getIteratorInfo()\n",
    "        print(f'Batch: {iterInfo[0]} / {iterInfo[1]}')\n",
    "        batch = loader.getNext()\n",
    "        (recognized, _) = model.inferBatch(batch)\n",
    "\n",
    "        print('Ground truth -> Recognized')\n",
    "        for i in range(len(recognized)):\n",
    "            numWordOK += 1 if batch.gtTexts[i] == recognized[i] else 0\n",
    "            numWordTotal += 1\n",
    "            dist = editdistance.eval(recognized[i], batch.gtTexts[i])\n",
    "            numCharErr += dist\n",
    "            numCharTotal += len(batch.gtTexts[i])\n",
    "            print('[OK]' if dist == 0 else '[ERR:%d]' % dist, '\"' + batch.gtTexts[i] + '\"', '->',\n",
    "                  '\"' + recognized[i] + '\"')\n",
    "\n",
    "    # print validation result\n",
    "    charErrorRate = numCharErr / numCharTotal\n",
    "    wordAccuracy = numWordOK / numWordTotal\n",
    "    print(f'Character error rate: {charErrorRate * 100.0}%. Word accuracy: {wordAccuracy * 100.0}%.')\n",
    "    return charErrorRate, wordAccuracy\n",
    "\n",
    "\n",
    "def infer(model, fnImg):\n",
    "    \"recognize text in image provided by file path\"\n",
    "    img = preprocess(cv2.imread(fnImg, cv2.IMREAD_GRAYSCALE), Model.imgSize)\n",
    "    batch = Batch(None, [img])\n",
    "    (recognized, probability) = model.inferBatch(batch, True)\n",
    "    print(f'Recognized: \"{recognized[0]}\"')\n",
    "    print(f'Probability: {probability[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "********\n",
      "Namespace(batch_size=100, data_dir=None, decoder='bestpath', dump=False, fast=False, train=False, validate=False)\n",
      "{'train': False, 'validate': False, 'decoder': 'bestpath', 'batch_size': 100, 'data_dir': None, 'fast': False, 'dump': False}\n",
      "********\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\legacy_tf_layers\\normalization.py:308: UserWarning: `tf.layers.batch_normalization` is deprecated and will be removed in a future version. Please use `tf.keras.layers.BatchNormalization` instead. In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.BatchNormalization` documentation).\n",
      "  '`tf.layers.batch_normalization` is deprecated and '\n",
      "C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1719: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
      "  warnings.warn('`layer.apply` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`tf.nn.rnn_cell.MultiRNNCell` is deprecated. This class is equivalent as `tf.keras.layers.StackedRNNCells`, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From C:\\Users\\mrkawa\\Documents\\Berkeley\\w210\\capstone-project\\SimpleHTR\\src\\Model.py:91: bidirectional_dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:447: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:981: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\legacy_rnn\\rnn_cell_impl.py:903: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
      "C:\\Users\\mrkawa\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer_v1.py:1727: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.6.12 |Anaconda, Inc.| (default, Sep  9 2020, 00:29:25) [MSC v.1916 64 bit (AMD64)]\n",
      "Tensorflow: 2.4.1\n",
      "Init with stored values from ../model/snapshot-39\n",
      "INFO:tensorflow:Restoring parameters from ../model/snapshot-39\n",
      "Recognized: \"Hello\"\n",
      "Probability: 0.42098307609558105\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"main function\"\n",
    "    sys.argv = []\n",
    "    parser = argparse.ArgumentParser(sys.argv)\n",
    "    parser.add_argument('--train', help='train the NN', action='store_true')\n",
    "    parser.add_argument('--validate', help='validate the NN', action='store_true')\n",
    "    parser.add_argument('--decoder', choices=['bestpath', 'beamsearch', 'wordbeamsearch'], default='bestpath',\n",
    "                        help='CTC decoder')\n",
    "    parser.add_argument('--batch_size', help='batch size', type=int, default=100)\n",
    "    parser.add_argument('--data_dir', help='directory containing IAM dataset', type=Path, required=False)\n",
    "    parser.add_argument('--fast', help='use lmdb to load images', action='store_true')\n",
    "    parser.add_argument('--dump', help='dump output of NN to CSV file(s)', action='store_true')\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "#     args.validate = True\n",
    "#     args.data_dir = Path('data')\n",
    "    print(\"\\n********\")\n",
    "    print(args)\n",
    "    print(vars(args))\n",
    "    print(\"********\\n\")\n",
    "\n",
    "    \n",
    "    # set chosen CTC decoder\n",
    "    if args.decoder == 'bestpath':\n",
    "        decoderType = DecoderType.BestPath\n",
    "    elif args.decoder == 'beamsearch':\n",
    "        decoderType = DecoderType.BeamSearch\n",
    "    elif args.decoder == 'wordbeamsearch':\n",
    "        decoderType = DecoderType.WordBeamSearch\n",
    "\n",
    "    # train or validate on IAM dataset\n",
    "    if args.train or args.validate:\n",
    "        # load training data, create TF model\n",
    "        loader = DataLoaderIAM(args.data_dir, args.batch_size, Model.imgSize, Model.maxTextLen, args.fast)\n",
    "\n",
    "        # save characters of model for inference mode\n",
    "        open(FilePaths.fnCharList, 'w').write(str().join(loader.charList))\n",
    "\n",
    "        # save words contained in dataset into file\n",
    "        open(FilePaths.fnCorpus, 'w').write(str(' ').join(loader.trainWords + loader.validationWords))\n",
    "\n",
    "        # execute training or validation\n",
    "        if args.train:\n",
    "            model = Model(loader.charList, decoderType)\n",
    "            train(model, loader)\n",
    "        elif args.validate:\n",
    "            model = Model(loader.charList, decoderType, mustRestore=True)\n",
    "            validate(model, loader)\n",
    "\n",
    "    # infer text on test image\n",
    "    else:\n",
    "        model = Model(open(FilePaths.fnCharList).read(), decoderType, mustRestore=True, dump=args.dump)\n",
    "        infer(model, FilePaths.fnInfer)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
